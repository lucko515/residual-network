{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used for this project/experiment is MNIST dataset. You can download it using tensorflow built-in functions as shown below.\n",
    "\n",
    "\n",
    "Network depth is of crucial importance in neural network architectures, but deeper networks are more difficult to train. The residual learning framework eases the training of these networks, and enables them to be substantially deeper — leading to improved performance in both visual and non-visual tasks. These residual networks are much deeper than their ‘plain’ counterparts, yet they require a similar number of parameters (weights).\n",
    "\n",
    "Materials:\n",
    "\n",
    "[Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "\n",
    "[Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027.pdf)\n",
    "\n",
    "This [Blog post](https://blog.waya.ai/deep-residual-learning-9610bb62c355) is great for intuition behind ResNet.\n",
    "\n",
    "![](paper_net.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_data = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(shape):\n",
    "    '''\n",
    "    Weights initialization helper function.\n",
    "    \n",
    "    Input(s): shape - Type: int list, Example: [5, 5, 32, 32], This parameter is used to define dimensions of weights tensor\n",
    "    \n",
    "    Output: tensor of weights in shape defined with the input to this function\n",
    "    '''\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bias_init(shape, bias_value=0.01):\n",
    "    '''\n",
    "    Bias initialization helper function.\n",
    "    \n",
    "    Input(s): shape - Type: int list, Example: [32], This parameter is used to define dimensions of bias tensor.\n",
    "              bias_value - Type: float number, Example: 0.01, This parameter is set to be value of bias tensor.\n",
    "    \n",
    "    Output: tensor of biases in shape defined with the input to this function\n",
    "    '''\n",
    "    return tf.Variable(tf.constant(bias_value, shape=shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_custom(input, filter_size, num_of_channels, num_of_filters, activation=tf.nn.relu, dropout=None,\n",
    "                  padding='SAME', max_pool=True, strides=(1, 1)):  \n",
    "    '''\n",
    "    This function is used to define a convolutional layer for a network,\n",
    "    \n",
    "    Input(s): input - this is input into convolutional layer (Previous layer or an image)\n",
    "              filter_size - also called kernel size, kernel is moved (convolved) across an image. Example: 3\n",
    "              number_of_channels - how many channels the input tensor has\n",
    "              number_of_filters - this is hyperparameter, and this will set one of dimensions of the output tensor from \n",
    "                                  this layer. Note: this number will be number_of_channels for the layer after this one\n",
    "              max_pool - if this is True, output tensor will be 2x smaller in size. Max pool is there to decrease spartial \n",
    "                        dimensions of our output tensor, so computation is less expensive.\n",
    "              padding - the way that we pad input tensor with zeros (\"SAME\" or \"VALID\")\n",
    "              activation - the non-linear function used at this layer.\n",
    "              \n",
    "              \n",
    "    Output: Convolutional layer with input parameters.\n",
    "    '''\n",
    "    weights = weights_init([filter_size, filter_size, num_of_channels, num_of_filters])\n",
    "    bias = bias_init([num_of_filters])\n",
    "    \n",
    "    layer = tf.nn.conv2d(input, filter=weights, strides=[1, strides[0], strides[1], 1], padding=padding) + bias\n",
    "    \n",
    "    if activation != None:\n",
    "        layer = activation(layer)\n",
    "    \n",
    "    if max_pool:\n",
    "        layer = tf.nn.max_pool(layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    if dropout != None:\n",
    "        layer = tf.nn.dropout(layer, dropout)\n",
    "        \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(layer):\n",
    "    '''\n",
    "    This method is used to convert convolutional output (4 dimensional tensor) into 2 dimensional tensor.\n",
    "    \n",
    "    Input(s): layer - the output from last conv layer in your network (4d tensor)\n",
    "    \n",
    "    Output(s): reshaped - reshaped layer, 2 dimensional matrix\n",
    "               elements_num - number of features for this layer\n",
    "    '''\n",
    "    shape = layer.get_shape()\n",
    "    \n",
    "    num_elements_ = shape[1:4].num_elements()\n",
    "    \n",
    "    flattened_layer = tf.reshape(layer, [-1, num_elements_])\n",
    "    return flattened_layer, num_elements_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_custom(input, input_size, output_size, activation=tf.nn.relu, dropout=None):\n",
    "    '''\n",
    "    This function is used to define a fully connected layer for a network,\n",
    "    \n",
    "    Input(s): input - this is input into fully connected (Dense) layer (Previous layer or an image)\n",
    "              input_size - how many neurons/features the input tensor has. Example: input.shape[1]\n",
    "              output_shape - how many neurons this layer will have\n",
    "              activation - the non-linear function used at this layer.    \n",
    "              dropout - the regularization method used to prevent overfitting. The way it works, we randomly turn off\n",
    "                        some neurons in this layer\n",
    "              \n",
    "    Output: fully connected layer with input parameters.\n",
    "    '''\n",
    "    weights = weights_init([input_size, output_size])\n",
    "    bias = bias_init([output_size])\n",
    "    \n",
    "    layer = tf.matmul(input, weights) + bias\n",
    "    \n",
    "    if activation != None:\n",
    "        layer = activation(layer)\n",
    "    \n",
    "    if dropout != None:\n",
    "        layer = tf.nn.dropout(layer, dropout)\n",
    "        \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resunit implemented in this notebook is explained in this [paper](https://arxiv.org/pdf/1603.05027.pdf).\n",
    "\n",
    "This is a picutre of the resunit used:\n",
    "\n",
    "![](resunit.jpeg?raw=true)\n",
    "\n",
    "Note: implemented version is B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def residual_unit(layer):\n",
    "    '''\n",
    "    Input(s): layer - conv layer before this res unit\n",
    "    \n",
    "    Output(s): ResUnit layer - implemented as described in the paper\n",
    "    '''\n",
    "    step1 = tf.layers.batch_normalization(layer)\n",
    "    step2 = tf.nn.relu(step1)\n",
    "    step3 = conv2d_custom(step2, 3, 32, 32, activation=None, max_pool=False) #32 number of feautres is hyperparam\n",
    "    step4 = tf.layers.batch_normalization(step3)\n",
    "    step5 = tf.nn.relu(step4)\n",
    "    step6 = conv2d_custom(step5, 3, 32, 32, activation=None, max_pool=False)\n",
    "    return layer + step6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Residual Network (ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, [None, 28, 28, 1], name='inputs')\n",
    "targets = tf.placeholder(tf.float32, [None, 10], name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_of_layers = 20\n",
    "between_strides = num_of_layers/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prev1 = conv2d_custom(inputs, 3, 1, 32, activation=None, max_pool=False)\n",
    "prev1 = tf.layers.batch_normalization(prev1)\n",
    "for i in range(5): # this number * between_strides = number_of_layers\n",
    "    for j in range(int(between_strides)):\n",
    "        prev1 = residual_unit(prev1)\n",
    "    #After 4 res units we perform strides 2x2, which will reduce data\n",
    "    perv1 = conv2d_custom(inputs, 3, 1, 32, activation=None, max_pool=False, strides=[2, 2])\n",
    "    prev1 = tf.layers.batch_normalization(prev1)\n",
    "#after all resunits we have last conv layer, than flattening and output layer\n",
    "last_conv = conv2d_custom(prev1, 3, 32, 10, activation=None, max_pool=False)\n",
    "flat, features = flatten(last_conv)\n",
    "output = dense_custom(flat, features, 10, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This part is for computing the accuracy of this model\n",
    "pred_y = tf.nn.softmax(output)\n",
    "pred_y_true = tf.argmax(pred_y, 1)\n",
    "y_true = tf.argmax(targets, 1)\n",
    "correct_prediction = tf.equal(pred_y_true, y_true)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loss function and optimizer\n",
    "cost = tf.reduce_mean((tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=targets)))\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Training and testing helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "total_number_trained = 0\n",
    "epochs = 5\n",
    "def optmizer():\n",
    "\n",
    "    for i in (range(epochs)):\n",
    "        epoch_loss = []\n",
    "        start_epoch = time.time()\n",
    "        for ii in range(mnist_data.train.num_examples//batch_size):\n",
    "            batch = mnist_data.train.next_batch(batch_size)\n",
    "            imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "            labs = batch[1]\n",
    "\n",
    "            dict_input = {inputs:imgs, targets:labs}\n",
    "\n",
    "            c, _ = session.run([cost, optimizer], feed_dict=dict_input)\n",
    "            epoch_loss.append(c)\n",
    "        print(\"Epoche: {}/{}\".format(i+1, epochs), \"| Training accuracy: \", session.run(accuracy, feed_dict=dict_input), \n",
    "              \"| Cost: {}\".format(np.mean(epoch_loss)), \" | Time for epoch: {:.2f}s\".format(time.time() - start_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size_valid = 1000\n",
    "def validate_model():\n",
    "    accuracy_per_batch = []\n",
    "    for ii in range(mnist_data.validation.num_examples//batch_size_valid):\n",
    "        batch = mnist_data.validation.next_batch(batch_size_valid)\n",
    "        imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "        labs = batch[1]\n",
    "\n",
    "        accuracy_per_batch.append(session.run(accuracy, feed_dict={inputs:imgs, targets:labs}))\n",
    "\n",
    "    print(\"Validation per batch accuracy {}\".format(accuracy_per_batch))\n",
    "    print(\"Test accuracy average: {:.2f}%\".format(np.mean(accuracy_per_batch)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size_test = 1000\n",
    "def test_model():\n",
    "    accuracy_per_batch = []\n",
    "    for ii in range(mnist_data.test.num_examples//batch_size_test):\n",
    "        batch = mnist_data.test.next_batch(batch_size_test)\n",
    "        imgs = batch[0].reshape((-1, 28, 28, 1))\n",
    "        labs = batch[1]\n",
    "\n",
    "        accuracy_per_batch.append(session.run(accuracy, feed_dict={inputs:imgs, targets:labs}))\n",
    "\n",
    "    print(\"Test per batch accuracy {}\".format(accuracy_per_batch))\n",
    "    print(\"Test accuracy average: {:.2f}%\".format(np.mean(accuracy_per_batch)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Train/Test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optmizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "session.close()\n",
    "#close the session after testing the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
